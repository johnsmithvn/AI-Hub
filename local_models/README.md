# ğŸ“ Local Models Directory

ThÆ° má»¥c nÃ y chá»©a táº¥t cáº£ AI models mÃ  báº¡n Ä‘Ã£ táº£i vÃ  copy thá»§ cÃ´ng vÃ o há»‡ thá»‘ng.

## ğŸ—‚ï¸ Cáº¥u trÃºc thÆ° má»¥c:

```
local_models/
â”œâ”€â”€ chat_models/          # Models cho chat general
â”‚   â”œâ”€â”€ llama2-7b/       # VÃ­ dá»¥: Llama2 7B
â”‚   â”œâ”€â”€ mistral-7b/      # VÃ­ dá»¥: Mistral 7B
â”‚   â””â”€â”€ qwen-7b/         # VÃ­ dá»¥: Qwen 7B (Vietnamese support)
â”œâ”€â”€ code_models/          # Models cho code generation
â”‚   â”œâ”€â”€ codellama-7b/    # VÃ­ dá»¥: CodeLlama 7B
â”‚   â”œâ”€â”€ deepseek-coder/  # VÃ­ dá»¥: DeepSeek Coder
â”‚   â””â”€â”€ starcoder/       # VÃ­ dá»¥: StarCoder
â”œâ”€â”€ vietnamese_models/    # Models tá»‘i Æ°u cho tiáº¿ng Viá»‡t
â”‚   â”œâ”€â”€ viallama/        # Custom Vietnamese Llama
â”‚   â”œâ”€â”€ phobert/         # PhoBERT Vietnamese
â”‚   â””â”€â”€ vimistral/       # Vietnamese Mistral
â””â”€â”€ custom_models/        # Models custom cá»§a báº¡n
    â”œâ”€â”€ my_finetuned_llama/
    â”œâ”€â”€ domain_specific_model/
    â””â”€â”€ experimental_model/
```

## ğŸ“¥ CÃ¡ch thÃªm model má»›i:

### 1. Copy model vÃ o Ä‘Ãºng thÆ° má»¥c:
```bash
# VÃ­ dá»¥: Copy Llama2 7B vÃ o chat_models
cp -r /path/to/llama2-7b-hf/ local_models/chat_models/llama2-7b/

# VÃ­ dá»¥: Copy CodeLlama vÃ o code_models  
cp -r /path/to/codellama-7b-hf/ local_models/code_models/codellama-7b/
```

### 2. Cáº¥u trÃºc bÃªn trong má»—i model folder:
```
model_name/
â”œâ”€â”€ config.json          # Model configuration (REQUIRED)
â”œâ”€â”€ tokenizer.json       # Tokenizer configuration
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ pytorch_model.bin    # Model weights (hoáº·c .safetensors)
â”œâ”€â”€ pytorch_model.bin.index.json
â”œâ”€â”€ generation_config.json
â””â”€â”€ special_tokens_map.json
```

### 3. Verify model sau khi copy:
Há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng scan vÃ  detect models má»›i khi restart.

## ğŸ¯ Model Types vÃ  Use Cases:

### Chat Models (chat_models/)
- **Llama2**: General purpose conversation
- **Mistral**: Fast and efficient chat
- **Qwen**: Multilingual support (Vietnamese)
- **Vicuna**: Instruction-following

### Code Models (code_models/)
- **CodeLlama**: Python, JavaScript, general coding
- **DeepSeek Coder**: Advanced code generation
- **StarCoder**: Multiple programming languages
- **WizardCoder**: Code explanation and debugging

### Vietnamese Models (vietnamese_models/)
- **ViLlama**: Vietnamese fine-tuned Llama
- **PhoBERT**: Vietnamese BERT
- **mBERT**: Multilingual BERT with Vietnamese
- **Custom Vietnamese**: Your own trained models

### Custom Models (custom_models/)
- Your fine-tuned models
- Domain-specific models
- Experimental models
- LoRA adapters

## ğŸ”§ Supported Model Formats:

- âœ… **HuggingFace Transformers** (.bin, .safetensors)
- âœ… **PyTorch** native models
- âœ… **GGML/GGUF** (with conversion)
- âœ… **ONNX** models
- âœ… **TensorFlow** (with conversion)

## ğŸ® CÃ¡ch sá»­ dá»¥ng:

### 1. Load model qua API:
```bash
curl -X POST "http://localhost:8000/api/v1/models/load" \
-H "Content-Type: application/json" \
-d '{
  "model_name": "llama2-7b",
  "quantization": "4bit"
}'
```

### 2. Chat vá»›i model:
```bash
curl -X POST "http://localhost:8000/api/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{
  "model": "llama2-7b",
  "messages": [{"role": "user", "content": "Xin chÃ o!"}]
}'
```

### 3. Code generation:
```bash
curl -X POST "http://localhost:8000/api/v1/chat/completions" \
-H "Content-Type: application/json" \
-d '{
  "model": "codellama-7b", 
  "messages": [{"role": "user", "content": "Write a Python function to sort array"}]
}'
```

## ğŸ“ Training Custom Models:

Báº¡n cÃ³ thá»ƒ fine-tune báº¥t ká»³ model nÃ o trong thÆ° má»¥c nÃ y:

```bash
curl -X POST "http://localhost:8000/api/v1/training/start" \
-H "Content-Type: application/json" \
-d '{
  "base_model": "llama2-7b",
  "dataset_path": "training_data/my_vietnamese_data.json",
  "output_dir": "custom_models/my_vietnamese_llama",
  "training_config": {
    "epochs": 3,
    "batch_size": 4,
    "learning_rate": 2e-4,
    "lora_r": 16
  }
}'
```

## ğŸ’¡ Tips & Best Practices:

1. **VRAM Management**: RTX 4060 Ti 16GB cÃ³ thá»ƒ cháº¡y:
   - 1x 13B model (4bit quantization)
   - 2x 7B models Ä‘á»“ng thá»i
   - 3-4x models nhá» hÆ¡n

2. **Model Naming**: Sá»­ dá»¥ng tÃªn descriptive:
   - `llama2-7b-chat-vietnamese`
   - `codellama-7b-python-specialist`
   - `mistral-7b-instruct-v0.2`

3. **Quantization**: 
   - 4bit: Tiáº¿t kiá»‡m VRAM nháº¥t, quality tá»‘t
   - 8bit: Balance giá»¯a speed vÃ  quality
   - 16bit: Best quality, tá»‘n VRAM nháº¥t

4. **Storage**: Model size trung bÃ¬nh:
   - 7B: ~13GB (full) / ~4GB (4bit)
   - 13B: ~25GB (full) / ~7GB (4bit)
   - 30B+: Cáº§n external storage

## ğŸ” Troubleshooting:

### Model khÃ´ng Ä‘Æ°á»£c detect:
1. Check `config.json` cÃ³ tá»“n táº¡i
2. Check folder structure Ä‘Ãºng
3. Restart application
4. Check logs: `tail -f logs/app.log`

### Out of memory:
1. Sá»­ dá»¥ng quantization (4bit/8bit)
2. Unload models khÃ¡c
3. Giáº£m max_length trong generation
4. Check GPU memory: `nvidia-smi`

### Slow generation:
1. Check GPU utilization
2. Tá»‘i Æ°u generation parameters
3. Sá»­ dá»¥ng smaller models cho dev/test
4. Enable torch.compile() (Python 3.11+)
